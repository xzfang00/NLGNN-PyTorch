这段代码中的非局部操作体现在 self.attention_layer 这一层。
这一层计算每个节点的注意力权重，然后将每个节点的特征向量与对应的注意力权重相乘，得到加权特征向量。
这一过程可以看作是计算每个位置与所有其他位置之间的关系，从而捕获数据中的长程依赖关系。
具体来说，在 forward 函数中，a = self.attention_layer(h) 这一行计算了每个节点的注意力权重，
然后 h = a * h 这一行将每个节点的特征向量与对应的注意力权重相乘，得到加权特征向量。